# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fA7dqHi_uX8DgV1DoJ4ryJw7MmbpHgOv

<h1> <u> Project </u> : Next Best Offer Recommendation System for Revenue Optimization in Telecommunications

<h2> Importing Necessary Libraries
"""

#!pip install tensorflow

#!pip install pandas

#!pip install numpy

#!pip install seaborn

#!pip install plotly

#!pip install sklearn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode,iplot

"""<h2> Data Loading and Eyeballing"""

data=pd.read_csv("data_summer_course.csv")
data.head()

data.duplicated().sum()

data.describe()

data['CUSTSEGMENT']=data['CUSTSEGMENT'].fillna('No_ne')

data.shape

data.info()

data.isnull().sum().sort_values(ascending=False)

def missing_value_percent(data):
    l=data.columns
    obj=list()
    num=list()
    for i,j in zip(l,data.dtypes):
        if j=="object":
            g=list()
            g.extend([i,((data[i].isnull().sum())*100)/data.shape[0]])
            g=tuple(g)
            obj.append(g)
        else:
            g=list()
            g.extend([i,((data[i].isnull().sum())*100)/data.shape[0]])
            g=tuple(g)
            num.append(g)
    obj.sort(key=lambda x:x[1],reverse=True)
    num.sort(key=lambda x:x[1],reverse =True)
    print("OBJECT -> ",len(obj))
    for i in range(len(obj)):
        print(obj[i])
    print("============================================")
    print("NUMERIC -> ",len(num))
    for j in range(len(num)):
        print(num[j])
    labels, values = zip(*obj)

    # Create a bar chart
    plt.figure(figsize=(10, 6))
    plt.bar(labels, values)
    plt.xticks(rotation=45, ha='right')
    plt.xlabel('Features')
    plt.ylabel('Null Value Percent')
    plt.title('Null Value Percent v/s Feature')
    plt.tight_layout()

    # Show the plot
    plt.show()
    labels, values = zip(*num)

    # Create a bar chart
    plt.figure(figsize=(10, 6))
    plt.bar(labels, values)
    plt.xticks(rotation=45, ha='right')
    plt.xlabel('Features')
    plt.ylabel('NUll value percent')
    plt.title('Null Value Percent v/s Feature')
    plt.tight_layout()

    # Show the plot
    plt.show()

missing_value_percent(data)

"""<h3> Conclusion From Initial Eyeballing </h3>

* There are 30 columns with 200000 data points
* There are 14 object type columns and 16 numeric type columns
* Out of all columns <b> top 5 </b> columns with highest null values are

  * HANDSETCHANGESFLAGD1_30 with <b> 82% null values </b> ,
  * last_app_used with <b> 60 % null values </b> ,
  * CNTCHURND1_30 with <b> 55% null values </b> ,
  * DEVICEMODELC , DEVICEDUALSIMFLAG , DEVICENETWORK , each with <b> 39 % null values </b>
  * SUMDATAUSG4GCD1_15 with <b> 25 % null values </b>

<h3> Things that can be done for next step </h3>

* Any Column with more than 50% null value can be removed
* To understand the significance of each column we need to perform exploratory data analysis on clean data
* After analysis we can select important columns and try to replace null values in those columns

<h2> Exploratory Data Analysis

To get a insight of number of null values in number of rows we can add a column <b> num_null_rows </b> which will store number of null values in that row
"""

data1=data.copy(deep=True)
def null_values_col(data2):
    null_values=data2.isnull().sum(axis=1)
    data2["null_value_cols"]=null_values
    return data2
data1=null_values_col(data1)
data1

data1.isnull().sum()

data1['null_value_cols'].describe()

f=data1['null_value_cols'].value_counts().to_dict()
sorted_data1 = {k: v for k, v in sorted(f.items())}
sorted_data1

"""


* There are rows with atmost 13 columns with missing data

"""

data1=data1.drop(['HANDSETCHANGESFLAGD1_30','last_app_used','CNTCHURND1_30'], axis=1)

data1

data1.isnull().sum().sort_values()

data2=data1.copy(deep=True)
data2.dropna(inplace=True,axis='index',subset='DEVICENETWORK')

data2.shape

"""To get any relevant information about data we have for now dropped all null values

once we get the important features we will start filling null values
"""

# Create a dictionary to map mobile numbers to unique user IDs

mobile_to_user_id = {mobile: user_id for user_id, mobile in enumerate(data2['Mobile_Number'].unique())}

# Map the 'Mobile_Number' column to the corresponding user IDs

data2['UserID'] = data2['Mobile_Number'].map(mobile_to_user_id)
data2=data2.drop(['Mobile_Number'],axis=1)
data2.head()

def data_dtype(data):
    obj=list()
    num=list()
    l=data.columns
    for i,j in zip(l,data.dtypes):
        if j=='object':
            obj.append(i)
        else:
            num.append(i)
    return obj,num

obj,num=data_dtype(data2)

obj

import plotly.io as pio

m=['DEVICETYPE' ,'ISDEVICE3GENABLED' ,'ISDEVICEDATAENABLED', 'CUSTSEGMENT' ,'DEVICENETWORK', 'SMARTPHONEFLAG','VASSUBSCRIBERFLAG' ]

def pie_chart_(data,i):
    dic=data[i].value_counts().to_dict()
    lables=list(dic.keys())
    values=list(dic.values())
    print(lables)
    print(values)
    s='Percent Compostion of '+i
    fig = px.pie(data, values=values, names=lables ,title = s)


    fig.show()
for i in m:
    pie_chart_(data2,i)

"""![pie_chart.jpg]

num

n=['ARPUD1_30','SUMVCEREVCD1_30','SUMDATAREVCD1_30','SUMDATAUSGCD1_30','SUMVOICEUSGCD1_30','MAINACTBAL1','SUMDATAUSG4GCD1_15',
 'SUMDATAUSG4GCD15_30']

# Explore distributions of numerical columns
numerical_cols = n
for col in numerical_cols:
    plt.figure(figsize=(8, 6))
    sns.histplot(data[col], kde=True, bins=20)
    plt.title(f'Distribution of {col}')
    plt.show()

jj1=[500,1000,1500,2000,2500,3000]
plt.figure(figsize=(10, 6))
ax=sns.boxplot(x='CUSTSEGMENT', y='ARPUD1_30', data=data2)
plt.title('Customer Segment vs. ARPUD1_30')

#plt.yticks(jj1)
plt.show()

correlation_matrix = data2[n].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Heatmap')
plt.show()

# Check the correlation between numerical variables and the target variable (ARPUD1_30)

correlation_with_target = data2.corr(numeric_only=True)['ARPUD1_30'].sort_values(ascending=False)
print(correlation_with_target)

sns.pairplot(data2,vars=['SUMDATAUSGCD1_30','ARPUD1_30'],kind='reg',hue='CUSTSEGMENT')

fig=px.scatter_matrix(data2, dimensions=['SUMDATAUSGCD1_30','ARPUD1_30'], color="CUSTSEGMENT")
fig.show()

#!pip install statsmodels

fig = px.scatter(data2, x="SUMDATAUSGCD1_30", y="ARPUD1_30", facet_col='DEVICENETWORK', color="CUSTSEGMENT", trendline="ols")
fig.show()

results = px.get_trendline_results(fig)
#print(results)
results.query("DEVICENETWORK == '4G' and CUSTSEGMENT == 'Silver'").px_fit_results.iloc[0].summary()

#SUMDATAREVCD1_30

fig = px.scatter(data2, x="SUMDATAREVCD1_30", y="ARPUD1_30", facet_col='DEVICENETWORK', color="CUSTSEGMENT", trendline="ols")
fig.show()

results = px.get_trendline_results(fig)
#print(results)
results.query("DEVICENETWORK == '4G' and CUSTSEGMENT == 'Silver'").px_fit_results.iloc[0].summary()

#SUMVCEREVCD1_30
fig = px.scatter(data2, x="SUMVCEREVCD1_30", y="ARPUD1_30", facet_col='DEVICENETWORK', color="CUSTSEGMENT", trendline="ols")
fig.show()

results = px.get_trendline_results(fig)
#print(results)
results.query("DEVICENETWORK == '4G' and CUSTSEGMENT == 'Silver'").px_fit_results.iloc[0].summary()

#SUMVOICEUSGCD1_30

fig = px.scatter(data2, x="SUMVOICEUSGCD1_30", y="ARPUD1_30", facet_col='DEVICENETWORK', color="CUSTSEGMENT", trendline="ols")
fig.show()

results = px.get_trendline_results(fig)
#print(results)
results.query("DEVICENETWORK == '4G' and CUSTSEGMENT == 'Silver'").px_fit_results.iloc[0].summary()

"""![pairplot.jpg]

<h3> Lets review correlation of '_days' with  ARPUD1_30
"""

data4=data.copy(deep=True)
data4=data4.dropna()
data4['network_custsegment']=data4['DEVICENETWORK'].astype(str) + "_" + data4['CUSTSEGMENT'].astype(str)

data4['LASTREVGENEVTDATE']=pd.to_datetime(data4['LASTREVGENEVTDATE'])

data4=data4.sort_values('LASTREVGENEVTDATE',ascending=True)

fig = px.scatter(data4, x='LASTREVGENEVTDATE', y='ARPUD1_30',color='network_custsegment')
fig.show()

import plotly.express as px

# Assuming df is your DataFrame with columns 'date', 'value', and 'CUSTSEGMENT'
fig = px.line(data4, x='LASTREVGENEVTDATE', y='ARPUD1_30', color='CUSTSEGMENT',
              title='Line Plot Grouped by LASTREVGENEVTDATE',
              labels={'date': 'Date', 'value': 'Value'})

fig.show()

data4['LASTRECHRGDATEE']=pd.to_datetime(data4['LASTRECHRGDATE'])

data4=data4.sort_values('LASTRECHRGDATE',ascending=True)

fig = px.scatter(data4, y='LASTRECHRGDATE', x='ARPUD1_30',color='network_custsegment')
fig.show()

"""![total_rev_date.jpg]

<h3> Graph to support above correlation</h3>

<h2> Data Cleaning </h2>

* We will be focusing more on cleaning and removing null values of  SUMDATAUSGCD1_30 , SUMVOICEUSGCD1_30 , SUMDATAREVCD1_30, SUMVCEREVCD1_30 , SUMDATAUSG4G1_15, SUMDATAUSG4G15_30 ,DEVICENETWORK, CUSTSEGMENT as these values play an import role later model .

* We will also convert date to days .

* We will add SUMDATAUSG4G1_15, SUMDATAUSG4G15_30 togather to make SUMDATAUSG4G
"""

from datetime import datetime

def days(data2, col_name):
    current_date_str = datetime.now().strftime('%m/%d/%Y')
    g = list()

    for x in data2[col_name]:
        if not pd.isnull(x):  # Skip null values in the col_name column
            curr_date = datetime.strptime(datetime.now().strftime('%m/%d/%Y'), '%m/%d/%Y')
            col_date = datetime.strptime(str(x)[:-9], '%m/%d/%Y')
            g.append((curr_date - col_date).days)
        else:
            g.append(None)  # If the value is null, append None to the result list

    data2[col_name + "_days"] = g
    print(col_name)
    return data2

data3=data.copy(deep=True)
#data2=data2.drop(['LASTVCEUSGDATE','FIRSTCALLDATE',"AGEONNETWORK","SUBSRIBERLASTBALANCE","last_app_used","DEVICEMODELC","CNTZEROBALCD_1","MAINACTBAL1"],axis=1)
x='LASTREVGENEVTDATE'
data3=days(data3,x).copy(deep=True)
data3=days(data3,"LASTRECHRGDATE").copy(deep=True)
data3=days(data3,"FIRSTREVGENEVTDATE").copy(deep=True)
data3=days(data3,'LASTVCEUSGDATE').copy(deep=True)
data3=days(data3,'FIRSTCALLDATE').copy(deep=True)
data3=data3.drop(["LASTREVGENEVTDATE","FIRSTREVGENEVTDATE",'LASTVCEUSGDATE','FIRSTCALLDATE','LASTRECHRGDATE'],axis=1).copy(deep=True)

data3

#data4.to_csv('data4_with_date.csv',index=False)

correlation_with_target = data3.corr(numeric_only=True)['ARPUD1_30'].sort_values(ascending=False).to_dict()

for i in ["LASTREVGENEVTDATE_days","FIRSTREVGENEVTDATE_days",'LASTVCEUSGDATE_days','FIRSTCALLDATE_days','LASTRECHRGDATE_days']:
    print(i," -> ",correlation_with_target[i])
#print(correlation_with_target)

"""- **'LASTREVGENEVTDATE_days' -> -0.3004516397754729**: This variable has a moderate negative correlation with 'ARPUD1_30'. As 'LASTREVGENEVTDATE_days' increases, 'ARPUD1_30' tends to decrease, and vice versa.

- **'FIRSTREVGENEVTDATE_days' -> 0.0048579435715423395**: This variable has a very weak positive correlation with 'ARPUD1_30'. Changes in 'FIRSTREVGENEVTDATE_days' are unlikely to have a significant effect on 'ARPUD1_30'.

- **'LASTVCEUSGDATE_days' -> -0.1897406264538312**: This variable has a weak negative correlation with 'ARPUD1_30'. As 'LASTVCEUSGDATE_days' increases, 'ARPUD1_30' tends to decrease slightly, and vice versa.

- **'FIRSTCALLDATE_days' -> 0.004822719245848176**: This variable has a very weak positive correlation with 'ARPUD1_30'. Changes in 'FIRSTCALLDATE_days' are unlikely to have a significant effect on 'ARPUD1_30'.

- **'LASTRECHRGDATE_days' -> -0.2076434833136745**: This variable has a weak negative correlation with 'ARPUD1_30'. As 'LASTRECHRGDATE_days' increases, 'ARPUD1_30' tends to decrease slightly, and vice versa.

Based on these correlations, **'LASTREVGENEVTDATE_days'** , **'LASTRECHRGDATE_days'** is likely the most important variable to consider when predicting 'ARPUD1_30', as it has the strongest correlation.

<h3> Findings </h3>

* For ARPUD1_30 we found that SUMDATAUSGCD1_30 , SUMVOICEUSGCD1_30 , SUMDATAREVCD1_30, SUMVCEREVCD1_30 are the columns which are highly correlated .

* To understand this coorelation with respect to DEVICENETWORK and  CUSTSEGMENT ,   in depth we drew required scatterplot with regression line in , to understand the trend .

<h4> Concluion with Date -> Days</h4>
    We can drop every date column except 'LASTREVGENEVTDATE_days' , 'LASTRECHRGDATE_days'.
"""

data3=data3.drop(["FIRSTREVGENEVTDATE_days",'LASTVCEUSGDATE_days','FIRSTCALLDATE_days','HANDSETCHANGESFLAGD1_30','last_app_used','CNTCHURND1_30','AGEONNETWORK','CNTZEROBALCD_1','SUBSRIBERLASTBALANCE','MAINACTBAL','MAINACTBAL1'],axis=1).copy(deep=True)
data3

"""We notice that

| Variable          | Count |
|-------------------|-------|
| DEVICENETWORK     | 61501 |
| DEVICEMODELC      | 61501 |
| DEVICEDUALSIMFLAG | 61501 |


have same number of null values , so lets check whether they are having null values in same rows
"""

data3.isnull().sum().sort_values()

def null_values_col(data2):
    null_values=data2.isnull().sum(axis=1)
    data2["null_value_cols"]=null_values
    return data2
k3=null_values_col(data3)
data3=k3.copy(deep=True)
data3

"""Remove all rows with more than 6 null values"""

data3=data3.dropna(axis='index',thresh=13)
#data3=data3[data3["null_value_cols"]<= 6 ].copy(deep=True)
data3.isnull().sum().sort_values()

data3['network_custsegment']=data3['DEVICENETWORK'].astype(str) + "_" + data3['CUSTSEGMENT'].astype(str)

data3

#data3 = data3.reset_index(drop=True)
data3[(data3['DEVICENETWORK'].isnull() ) & (data3['DEVICEMODELC'].isnull()) & (data['DEVICEDUALSIMFLAG'].isnull())]

"""<h4> Findings </h4>

each row which have a null value in DEVICENETWORK  has a null value in DEVICEMODELC and DEVICEDUALSIMFLAG , meaning we cant use this columns to guess null values in DEVICENETWORK.
"""

data3=data3.drop(['DEVICEMODELC','DEVICEDUALSIMFLAG'],axis=1).copy(deep=True)

"""<h3> Adding SUMDATAUSG4G1_15, SUMDATAUSG4G15_30 togather to make SUMDATAUSG4G"""

def sum_4g_use(data3):
    data3["SUMDATAUSG4GCD15_30"].fillna(0,inplace=True)
    data3["SUMDATAUSG4GCD1_15"].fillna(0,inplace=True)
    data3["SUMDATAUSG4G"]=data3["SUMDATAUSG4GCD15_30"]+data3["SUMDATAUSG4GCD1_15"]
    return data3
data3=sum_4g_use(data3)
data3=data3.drop(['SUMDATAUSG4GCD15_30',"SUMDATAUSG4GCD1_15"],axis=1)
data3.head()

mobile_to_user_id = {mobile: user_id for user_id, mobile in enumerate(data3['Mobile_Number'].unique())}

# Map the 'Mobile_Number' column to the corresponding user IDs

data3['UserID'] = data3['Mobile_Number'].map(mobile_to_user_id)
data3=data3.drop(['Mobile_Number'],axis=1)
data3

"""<h3> Fill null values in SUMDATAUSGCD1_30"""

data3[data3['SUMDATAUSGCD1_30'].isnull()]

median_1g_none=data3[(data3['network_custsegment']=='1G_No_ne') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()

data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='1G_No_ne'),'SUMDATAUSGCD1_30']=data3.fillna(median_1g_none)

median_1g_basic=data3[(data3['network_custsegment']=='1G_Basic') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='1G_Basic'),'SUMDATAUSGCD1_30']=data3.fillna(median_1g_basic)

median_1g_gold=data3[(data3['network_custsegment']=='1G_Gold') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='1G_Gold'),'SUMDATAUSGCD1_30']=data3.fillna(median_1g_gold)

median_1g_platinum=data3[(data3['network_custsegment']=='1G_Platinum') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='1G_Platinum'),'SUMDATAUSGCD1_30']=data3.fillna(median_1g_platinum)

median_2g_none=data3[(data3['network_custsegment']=='2G_No_ne') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='2G_No_ne'),'SUMDATAUSGCD1_30']=data3.fillna(median_2g_none)

median_2g_Basic=data3[(data3['network_custsegment']=='2G_Basic') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='2G_Basic'),'SUMDATAUSGCD1_30']=data3.fillna(median_2g_Basic)

median_2g_Silver=data3[(data3['network_custsegment']=='2G_Silver') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='2G_Silver'),'SUMDATAUSGCD1_30']=data3.fillna(median_2g_Silver)

median_2g_Gold=data3[(data3['network_custsegment']=='2G_Gold') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='2G_Gold'),'SUMDATAUSGCD1_30']=data3.fillna(median_2g_Gold)

median_3g_none=data3[(data3['network_custsegment']=='3G_No_ne') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>=0)]['SUMDATAUSGCD1_30'].median()
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='3G_No_ne'),'SUMDATAUSGCD1_30']=data3.fillna(median_3g_none)

median_3g_basic=data3[(data3['network_custsegment']=='3G_Basic') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>=0)]['SUMDATAUSGCD1_30'].median()
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='3G_Basic'),'SUMDATAUSGCD1_30']=data3.fillna(median_3g_basic)

p1=data3[(data3['network_custsegment']=='4G_No_ne') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()
p2=data3[(data3['network_custsegment']=='4G_No_ne') & (data3['SUMDATAUSG4G']>0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSG4G'].median()
median_4g_none=(p1+p2)/2
print(median_4g_none,p1,p2)

median_4g_none=data3[(data3['network_custsegment']=='4G_No_ne') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='4G_No_ne'),'SUMDATAUSGCD1_30']=data3.fillna(median_4g_none)

p1=data3[(data3['network_custsegment']=='4G_Basic') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()
p2=data3[(data3['network_custsegment']=='4G_Basic') & (data3['SUMDATAUSG4G']>0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSG4G'].median()
median_4g_basic=(p1+p2)/2
print(p1,p2,median_4g_basic,median_4g_basic>p1)

median_4g_basic=data3[(data3['network_custsegment']=='4G_Basic') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='4G_Basic'),'SUMDATAUSGCD1_30']=data3.fillna(median_4g_basic)

p1=data3[(data3['network_custsegment']=='4G_Gold') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()
p2=data3[(data3['network_custsegment']=='4G_Gold') & (data3['SUMDATAUSG4G']>0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSG4G'].median()
median_4g_gold=(p1+p2)/2
print(p1,p2,median_4g_gold)

p1=data3[(data3['network_custsegment']=='4G_Silver') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()
p2=data3[(data3['network_custsegment']=='4G_Silver') & (data3['SUMDATAUSG4G']>0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSG4G'].median()
median_4g_silver=(p1+p2)/2
print(p1,p2,median_4g_gold)

data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='4G_Silver'),'SUMDATAUSGCD1_30']=data3.fillna(median_4g_silver)

median_4g_gold=data3[(data3['network_custsegment']=='4G_Gold') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>=0)]['SUMDATAUSGCD1_30'].median()
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='4G_Gold'),'SUMDATAUSGCD1_30']=data3.fillna(median_4g_gold)

p1=data3[(data3['network_custsegment']=='4G_Platinum') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>=0)]['SUMDATAUSGCD1_30'].median()
p2=data3[(data3['network_custsegment']=='4G_Platinum') & (data3['SUMDATAUSG4G']>0) &(data3['SUMDATAUSGCD1_30']>=0)]['SUMDATAUSG4G'].median()
median_4g_platina=(p1+p2)/2
print(median_4g_platina,p1,p2)

median_4g_platinum=data3[(data3['network_custsegment']=='4G_Platinum') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>=0)]['SUMDATAUSGCD1_30'].median()
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='4G_Platinum'),'SUMDATAUSGCD1_30']=data3.fillna(median_4g_platina)

median_5g_none=data3[(data3['network_custsegment']=='5G_No_ne') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='5G_No_ne'),'SUMDATAUSGCD1_30']=data3.fillna(median_5g_none)

median_5g_basic=data3[(data3['network_custsegment']=='5G_Basic') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='5G_Basic'),'SUMDATAUSGCD1_30']=data3.fillna(median_5g_basic)

median_5g_silver=data3[(data3['network_custsegment']=='5G_Silver') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='5G_Silver'),'SUMDATAUSGCD1_30']=data3.fillna(median_5g_silver)

median_5g_gold=data3[(data3['network_custsegment']=='5G_Gold') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='5G_Gold'),'SUMDATAUSGCD1_30']=data3.fillna(median_5g_gold)

median_5g_platinum=data3[(data3['network_custsegment']=='5G_Platinum') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>=0)]['SUMDATAUSGCD1_30'].median()
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='5G_Platinum'),'SUMDATAUSGCD1_30']=data3.fillna(median_5g_platinum)

median_1g_silver=data3[(data3['network_custsegment']=='1G_Silver') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()
median_2g_basic=data3[(data3['network_custsegment']=='2G_Basic') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>=0)]['SUMDATAUSGCD1_30'].median()


data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='1G_Silver'),'SUMDATAUSGCD1_30']=data3.fillna(median_1g_silver)
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='2G_Basic'),'SUMDATAUSGCD1_30']=data3.fillna(median_2g_basic)

median_3g_silver=data3[(data3['network_custsegment']=='3G_Silver') & (data3['SUMDATAUSG4G']==0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSGCD1_30'].median()

data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='3G_Silver'),'SUMDATAUSGCD1_30']=data3.fillna(median_3g_silver)

data3.isnull().sum().sort_values()

"""<h3> Lets fill null values of SUMVOICEUSGCD1_30</h3>

"""

median_1g_none=data3[(data3['network_custsegment']=='1G_No_ne')  &(data3['SUMVOICEUSGCD1_30']>0)]['SUMVOICEUSGCD1_30'].median()
data3.loc[(data3['network_custsegment']=='1G_No_ne'),'SUMVOICEUSGCD1_30']=data3.fillna(median_1g_none)

median_1g_basic=data3[(data3['network_custsegment']=='1G_Basic') & (data3['SUMVOICEUSGCD1_30']>0)]['SUMVOICEUSGCD1_30'].median()
data3.loc[(data3['network_custsegment']=='1G_Basic'),'SUMVOICEUSGCD1_30']=data3.fillna(median_1g_basic)

median_1g_gold=data3[(data3['network_custsegment']=='1G_Gold') &(data3['SUMVOICEUSGCD1_30']>0)]['SUMVOICEUSGCD1_30'].median()
data3.loc[(data3['SUMDATAUSG4G'] == 0) &(data3['network_custsegment']=='1G_Gold'),'SUMVOICEUSGCD1_30']=data3.fillna(median_1g_gold)

median_1g_platinum=data3[(data3['network_custsegment']=='1G_Platinum')  &(data3['SUMVOICEUSGCD1_30']>0)]['SUMVOICEUSGCD1_30'].median()
data3.loc[(data3['network_custsegment']=='1G_Platinum'),'SUMVOICEUSGCD1_30']=data3.fillna(median_1g_platinum)

median_2g_none=data3[(data3['network_custsegment']=='2G_No_ne')  &(data3['SUMVOICEUSGCD1_30']>0)]['SUMVOICEUSGCD1_30'].median()
data3.loc[(data3['network_custsegment']=='2G_No_ne'),'SUMVOICEUSGCD1_30']=data3.fillna(median_2g_none)

median_2g_Basic=data3[(data3['network_custsegment']=='2G_Basic')  &(data3['SUMVOICEUSGCD1_30']>0)]['SUMVOICEUSGCD1_30'].median()
data3.loc[(data3['network_custsegment']=='2G_Basic'),'SUMVOICEUSGCD1_30']=data3.fillna(median_2g_Basic)


median_2g_Silver=data3[(data3['network_custsegment']=='2G_Silver')  &(data3['SUMVOICEUSGCD1_30']>0)]['SUMVOICEUSGCD1_30'].median()
data3.loc[(data3['network_custsegment']=='2G_Silver'),'SUMVOICEUSGCD1_30']=data3.fillna(median_2g_Silver)

median_2g_Gold=data3[(data3['network_custsegment']=='2G_Gold') &(data3['SUMVOICEUSGCD1_30']>0)]['SUMVOICEUSGCD1_30'].median()
data3.loc[(data3['network_custsegment']=='2G_Gold'),'SUMVOICEUSGCD1_30']=data3.fillna(median_2g_Gold)

median_3g_none=data3[(data3['network_custsegment']=='3G_No_ne')  &(data3['SUMVOICEUSGCD1_30']>=0)]['SUMDATAUSGCD1_30'].median()
data3.loc[(data3['network_custsegment']=='3G_No_ne'),'SUMVOICEUSGCD1_30']=data3.fillna(median_3g_none)


median_3g_basic=data3[(data3['network_custsegment']=='3G_Basic')  &(data3['SUMVOICEUSGCD1_30']>=0)]['SUMVOICEUSGCD1_30'].median()
data3.loc[(data3['network_custsegment']=='3G_Basic'),'SUMVOICEUSGCD1_30']=data3.fillna(median_3g_basic)



median_4g_none=data3[(data3['network_custsegment']=='4G_No_ne') &(data3['SUMVOICEUSGCD1_30']>0)]['SUMVOICEUSGCD1_30'].median()
print(median_4g_none,p1,p2)


data3.loc[(data3['network_custsegment']=='4G_No_ne'),'SUMVOICEUSGCD1_30']=data3.fillna(median_4g_none)



median_4g_basic=data3[(data3['network_custsegment']=='4G_Basic')  &(data3['SUMVOICEUSGCD1_30']>0)]['SUMVOICEUSGCD1_30'].median()
print(p1,p2,median_4g_basic,median_4g_basic>p1)


data3.loc[(data3['network_custsegment']=='4G_Basic'),'SUMVOICEUSGCD1_30']=data3.fillna(median_4g_basic)

p1=data3[(data3['network_custsegment']=='4G_Gold')  &(data3['SUMVOICEUSGCD1_30']>0)]['SUMVOICEUSGCD1_30'].median()
p2=data3[(data3['network_custsegment']=='4G_Gold') & (data3['SUMDATAUSG4G']>0) &(data3['SUMDATAUSGCD1_30']>0)]['SUMDATAUSG4G'].median()
median_4g_gold=data3[(data3['network_custsegment']=='4G_Gold')  &(data3['SUMVOICEUSGCD1_30']>0)]['SUMVOICEUSGCD1_30'].median()
#print(p1,p2,median_4g_gold)
data3.loc[(data3['network_custsegment']=='4G_Gold'),'SUMVOICEUSGCD1_30']=data3.fillna(median_4g_gold)


median_4g_silver=data3[(data3['network_custsegment']=='4G_Silver')  &(data3['SUMVOICEUSGCD1_30']>0)]['SUMVOICEUSGCD1_30'].median()
#print(p1,p2,median_4g_gold)
data3.loc[(data3['network_custsegment']=='4G_Silver'),'SUMVOICEUSGCD1_30']=data3.fillna(median_4g_silver)





median_4g_platina=data3[(data3['network_custsegment']=='4G_Platinum')  &(data3['SUMVOICEUSGCD1_30']>=0)]['SUMVOICEUSGCD1_30'].median()

data3.loc[(data3['network_custsegment']=='4G_Platinum'),'SUMVOICEUSGCD1_30']=data3.fillna(median_4g_platina)

median_5g_none=data3[(data3['network_custsegment']=='5G_No_ne')  &(data3['SUMVOICEUSGCD1_30']>0)]['SUMVOICEUSGCD1_30'].median()
data3.loc[(data3['network_custsegment']=='5G_No_ne'),'SUMVOICEUSGCD1_30']=data3.fillna(median_5g_none)


median_5g_basic=data3[(data3['network_custsegment']=='5G_Basic')  &(data3['SUMVOICEUSGCD1_30']>0)]['SUMVOICEUSGCD1_30'].median()
data3.loc[(data3['network_custsegment']=='5G_Basic'),'SUMVOICEUSGCD1_30']=data3.fillna(median_5g_basic)


median_5g_silver=data3[(data3['network_custsegment']=='5G_Silver') &(data3['SUMVOICEUSGCD1_30']>0)]['SUMVOICEUSGCD1_30'].median()
data3.loc[(data3['network_custsegment']=='5G_Silver'),'SUMVOICEUSGCD1_30']=data3.fillna(median_5g_silver)

median_5g_gold=data3[(data3['network_custsegment']=='5G_Gold')  &(data3['SUMVOICEUSGCD1_30']>0)]['SUMVOICEUSGCD1_30'].median()
data3.loc[(data3['network_custsegment']=='5G_Gold'),'SUMVOICEUSGCD1_30']=data3.fillna(median_5g_gold)

median_5g_platinum=data3[(data3['network_custsegment']=='5G_Platinum')  &(data3['SUMVOICEUSGCD1_30']>=0)]['SUMVOICEUSGCD1_30'].median()
data3.loc[(data3['network_custsegment']=='5G_Platinum'),'SUMVOICEUSGCD1_30']=data3.fillna(median_5g_platinum)

median_1g_silver=data3[(data3['network_custsegment']=='1G_Silver')  &(data3['SUMVOICEUSGCD1_30']>0)]['SUMVOICEUSGCD1_30'].median()
median_2g_basic=data3[(data3['network_custsegment']=='2G_Basic')  &(data3['SUMVOICEUSGCD1_30']>=0)]['SUMVOICEUSGCD1_30'].median()


data3.loc[(data3['network_custsegment']=='1G_Silver'),'SUMVOICEUSGCD1_30']=data3.fillna(median_1g_silver)
data3.loc[(data3['network_custsegment']=='2G_Basic'),'SUMVOICEUSGCD1_30']=data3.fillna(median_2g_basic)

median_3g_silver=data3[(data3['network_custsegment']=='3G_Silver')  &(data3['SUMVOICEUSGCD1_30']>0)]['SUMVOICEUSGCD1_30'].median()

data3.loc[(data3['network_custsegment']=='3G_Silver'),'SUMVOICEUSGCD1_30']=data3.fillna(median_3g_silver)

data3.isnull().sum().sort_values()

"""<h3> Replacing Null Values of SUMDATAREVCD1_30"""

data3.corr(numeric_only=True)['SUMDATAREVCD1_30'].sort_values(ascending=False)

"""<h3> Using Regression to replace null values"""

df_total=data3[['SUMDATAREVCD1_30','ARPUD1_30','SUMDATAUSGCD1_30','SUMDATAUSG4G']].copy(deep=True)
df_null=df_total[df_total['SUMDATAREVCD1_30'].isnull()].copy(deep=True)
df_train=df_total.copy(deep=True)
df_train=df_train.dropna(subset='SUMDATAREVCD1_30')

df_train

df_null

"""* For purpose of training we separated null values .
* We will train model on df_train and replace nan values in df_null
"""

#!pip install sklearn

from sklearn.model_selection import train_test_split ,GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor
import time

X=df_train[['ARPUD1_30' ,'SUMDATAUSGCD1_30','SUMDATAUSG4G']]
y=df_train[['SUMDATAREVCD1_30']]
# Train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Standardise the dataset
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

start_time = time.time()
# Create a Linear Regression model
model = LinearRegression()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
end_time = time.time()
print("Total Time for Linear regression was ",round(end_time-start_time,2))

start_time = time.time()
from sklearn.linear_model import Ridge
model = Ridge()

# Define the grid of hyperparameters to search
hyperparameters = {'alpha': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]}

# Set up the grid search
grid_search = GridSearchCV(model, hyperparameters, cv=5)

# Conduct the grid search
grid_search.fit(X_train, y_train)

# Print the best parameters
print(f'Best parameters: {grid_search.best_params_}')

# Make predictions with the best model
y_pred = grid_search.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)

print(f'Mean Squared Error: {mse}')

end_time = time.time()
print("Total Time for Ridge Linear regression was ",round(end_time-start_time,2))

"""<h3> Findings of Linear Regression </h3>

Using simple linear regression and ridge regression we found that mean square error on test data is 40.85

<h3> Using XgBoost </h3>

### XGBoost

XGBoost, short for "Extreme Gradient Boosting", is an open-source software library that provides a gradient boosting framework for languages such as Python, R, and Java. It is known for its speed and performance.

Gradient boosting is a machine learning technique that produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. XGBoost improves upon the base Gradient Boosting Machines (GBM) framework through systems optimization and algorithmic enhancements.


The final prediction model is a linear combination of many decision trees that are added during each iteration of the gradient boosting algorithm.

### Key Features of XGBoost

- **Regularization**: XGBoost has an option to penalize complex models through both L1 (Lasso Regression) and L2 (Ridge Regression) regularization to prevent overfitting.

- **Handling Missing Values**: XGBoost has an in-built routine to handle missing values.

- **Tree Pruning**: Unlike GBM, where tree pruning stops once a negative loss is encountered, XGBoost grows the tree up to max_depth and then prunes backward until the improvement in loss function is below a threshold.

- **Built-in Cross-Validation**: XGBoost allows a user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.

- **Parallel Processing**: XGBoost implements parallel processing and is faster than GBM. It also supports distributed computing and can handle very large datasets that do not fit into memory.

<pre>
start_time = time.time()
from sklearn.model_selection import RandomizedSearchCV
#### Create an XGBRegressor model
model_xgb = XGBRegressor()
#### Define the grid of hyperparameters to search
hyperparameters = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.3],
    'max_depth': [2, 4, 8],
    'colsample_bytree': [0.3, 0.7]
}
#### Set up the randomized search with cross-validation
randomized_search = RandomizedSearchCV(model_xgb, hyperparameters, n_iter=10, cv=5, verbose=1, n_jobs=-1)
#### Conduct the randomized search
randomized_search.fit(X_train, y_train)
#### Print the best parameters
print(f'Best parameters: {randomized_search.best_params_}')
#### Make predictions with the best model
y_pred = randomized_search.predict(X_test)
#### Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
end_time = time.time()
print("Total Time for XGBOOST with randomised search was ",round(end_time-start_time,2)," seconds")
</pre>
"""

from xgboost import XGBRegressor

# Create an XGBRegressor model with the specified parameters
model = XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.3, colsample_bytree=0.7)

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)

print(f'Mean Squared Error: {mse}')

from keras.models import Sequential
from keras.layers import Dense
import tensorflow as tf

X_train_temp, X_test, y_train_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_temp, y_train_temp, test_size=0.25, random_state=42)

"""<pre>
start_time = time.time()
tf.random.set_seed(42)
norm_layer=tf.keras.layers.Normalization(input_shape=X_train.shape[1:])
model=tf.keras.Sequential([
    norm_layer,
    tf.keras.layers.Dense(50,activation="relu"),
    tf.keras.layers.Dense(50,activation='relu'),
    tf.keras.layers.Dense(50,activation='relu'),
    tf.keras.layers.Dense(1)
])
optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3)
model.compile(loss='mse',optimizer=optimizer,metrics=['RootMeanSquaredError'])
norm_layer.adapt(X_train)
history=model.fit(X_train,y_train,epochs=100,validation_data=(X_valid,y_valid))
mse_test,rmse_test=model.evaluate(X_test,y_test)
print(mse_test,rmse_test)
end_time = time.time()
print("Total Time for Deep Learning  was ",round(end_time-start_time,2)," seconds")
</pre>

![deep_learning_1.png]

<h3>Conclusion for ' SUMDATAREVCD1_30' </h3>

We will use XGBoost to predict nan values

<h3> Finding Using ML algorithms </h3>

| ML Algorithm             | MSE   | Time   |
|--------------------------|-------|--------|
| Linear Regression        | 40.85 | 0.01 s |
| Ridge Linear Regression  | 40.85 | 0.3 s  |
| XGBoost                  | 33.26 | 23.57 s|
"""

df_null[['ARPUD1_30','SUMDATAUSGCD1_30','SUMDATAUSG4G']]

X_pred_null=df_null[['ARPUD1_30','SUMDATAUSGCD1_30','SUMDATAUSG4G']].copy(deep=True)
X_pred_null=scaler.fit_transform(X_pred_null)
y_pred_null=model.predict(X_pred_null)
y_pred_null

X_pred_null

nan_indices = data3[data3['SUMDATAREVCD1_30'].isnull()].index
data3.loc[nan_indices, 'SUMDATAREVCD1_30'] = y_pred_null
data['SUMDATAREVCD1_30'].fillna(pd.Series(y_pred), inplace=True)
data3

data3['SUMDATAREVCD1_30'].describe()

"""<h3> Replacing Null Values of SUMVCEREVCD1_30 </h3>"""

data3.isnull().sum().sort_values()

data3.corr(numeric_only=True)['SUMVCEREVCD1_30'].sort_values(ascending=False)

df_total=data3.copy(deep=True)
df_null=df_total[df_total['SUMVCEREVCD1_30'].isnull()].copy(deep=True)
df_train=df_total.copy(deep=True)
df_train=df_train.dropna(subset='SUMVCEREVCD1_30')

"""<h3> Initially all columns were used but after linear regression coefficients findings we are using the significant one only </h3>
all columns 'ARPUD1_30','SUMVOICEUSGCD1_30','SUMDATAREVCD1_30','SUMDATAUSGCD1_30','SUMDATAUSG4G','LASTREVGENEVTDATE_days'

that is 'ARPUD1_30' and 'SUMDATAUSGCD1_30'


"""

X=df_train[['ARPUD1_30','SUMVOICEUSGCD1_30','SUMDATAREVCD1_30','SUMDATAUSGCD1_30','SUMDATAUSG4G','LASTREVGENEVTDATE_days']]
y=df_train[['SUMVCEREVCD1_30']]
# Train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Standardise the dataset
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

y_test.shape

from sklearn.linear_model import LinearRegression
start_time = time.time()
#### Assume X is your feature matrix and y are your targets
model = LinearRegression()
model.fit(X_train, y_train)

#### Get coefficients
coefficients = model.coef_
y_pred=model.predict(X_test)
#### summarize coefficients
#for i, coef in enumerate(coefficients):
    #print('Feature: %0d, Coefficient: %.5f' % (i, coef))

mse = mean_squared_error(y_test, y_pred)

print(coefficients)

print(f'Mean Squared Error: {mse}')

end_time = time.time()

print("Total Time for Linear Regression  was ",round(end_time-start_time,2)," seconds")

"""<h3> Findings Using Linear Regression Coefficint </h3>

| Feature ID | Feature Name          | Coefficient    |
|------------|-----------------------|----------------|
| 0          | ARPUD1_30             | 38.2558719     |
| 1          | SUMVOICEUSGCD1_30     | -0.00927612937 |
| 2          | SUMDATAREVCD1_30      | 1.74533462     |
| 3          | SUMDATAUSGCD1_30      | -37.7471213    |
| 4          | SUMDATAUSG4G          | 8.44713859     |
| 5          | LASTREVGENEVTDATE_days| -8.83465082    |

"""

start_time = time.time()
from sklearn.linear_model import Ridge
model = Ridge()

# Define the grid of hyperparameters to search
hyperparameters = {'alpha': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]}

# Set up the grid search
grid_search = GridSearchCV(model, hyperparameters, cv=5)

# Conduct the grid search
grid_search.fit(X_train, y_train)

# Print the best parameters
print(f'Best parameters: {grid_search.best_params_}')

# Make predictions with the best model
y_pred = grid_search.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)

print(f'Mean Squared Error: {mse}')

end_time = time.time()
print("Total Time for Ridge Linear regression was ",round(end_time-start_time,2))

#list(range(0.1,0.9,0.5))

"""<pre>
start_time = time.time()
from sklearn.model_selection import RandomizedSearchCV

#### Create an XGBRegressor model
model_xgb = XGBRegressor()

#### Define the grid of hyperparameters to search
hyperparameters = {
    'n_estimators': [50, 100, 150, 200],
    'learning_rate': [0.01, 0.1, 0.2,0.25, 0.3],
    'max_depth': [2, 4, 6, 8,10],
    'colsample_bytree': np.arange(0.5, 0.9, 0.5).tolist()
}

#### Set up the randomized search with cross-validation
randomized_search = RandomizedSearchCV(model_xgb, hyperparameters, n_iter=10, cv=5, verbose=1, n_jobs=-1)

#### Conduct the randomized search
randomized_search.fit(X_train, y_train)

#### Print the best parameters
print(f'Best parameters: {randomized_search.best_params_}')

#### Make predictions with the best model
y_pred = randomized_search.predict(X_test)

#### Evaluate the model
mse = mean_squared_error(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
end_time = time.time()
print("Total Time for XGBOOST with randomised search was ",round(end_time-start_time,2)," seconds")
</pre>

![Xgb_para_voice_rev.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA+gAAABoCAYAAABi+

from xgboost import XGBRegressor
start_time = time.time()
# Assume X is your feature matrix and y are your targets
model_xgb = XGBRegressor()
model_xgb.fit(X_train, y_train)

# Get feature importance
importance = model_xgb.feature_importances_

# summarize feature importance
for i,v in enumerate(importance):
    print('Feature: %0d, Score: %.5f' % (i,v))
# Make predictions
y_pred = model_xgb.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
end_time = time.time()

print("Total Time for XGBOOST was ",round(end_time-start_time,2)," seconds")

"""4796.83      5734.023977928031"""

X_train_temp, X_test, y_train_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_temp, y_train_temp, test_size=0.25, random_state=42)

"""<pre>
start_time = time.time()
tf.random.set_seed(42)
norm_layer=tf.keras.layers.Normalization(input_shape=X_train.shape[1:])
model=tf.keras.Sequential([
    norm_layer,
    tf.keras.layers.Dense(50,activation="relu"),
    tf.keras.layers.Dense(50,activation='relu'),
    tf.keras.layers.Dense(50,activation='relu'),
    tf.keras.layers.Dense(10,activation='relu'),
    tf.keras.layers.Dense(5,activation='relu'),
    tf.keras.layers.Dense(1)
])
optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3)
model.compile(loss='mse',optimizer=optimizer,metrics=['RootMeanSquaredError'])
norm_layer.adapt(X_train)
history=model.fit(X_train,y_train,epochs=100,validation_data=(X_valid,y_valid))
mse_test,rmse_test=model.evaluate(X_test,y_test)
print(mse_test,rmse_test)
end_time = time.time()
print("Total Time for Deep Learning  was ",round(end_time-start_time,2)," seconds")
</pre>

![deep_learning_2_voic_rev.png]

<pre>
model.summary()
</pre>

![deep_learning_2_summary.png]

<h3> Conclusion For SUMVCEREVCD1_30 null value imputation </h3>

| ML Algorithm       | MSE       | Time   |
|--------------------|-----------|--------|
| Deep Learning      | 5278.811  | 263.83s|
| Linear Regression  | 7220.75   | 0.01 s |
| XGBoost            | 4797.84   | 1.86 s |


We will use <b> XGBoost </b>
"""

X_pred_null=df_null[['ARPUD1_30','SUMVOICEUSGCD1_30','SUMDATAREVCD1_30','SUMDATAUSGCD1_30','SUMDATAUSG4G','LASTREVGENEVTDATE_days']].copy(deep=True)
X_pred_null=scaler.fit_transform(X_pred_null)
y_pred_null=model_xgb.predict(X_pred_null)
y_pred_null

nan_indices = data3[data3['SUMVCEREVCD1_30'].isnull()].index
data3.loc[nan_indices, 'SUMVCEREVCD1_30'] = y_pred_null
data['SUMVCEREVCD1_30'].fillna(pd.Series(y_pred), inplace=True)
data3

data3['SUMVCEREVCD1_30'].describe()

data3.loc[data3['SUMVCEREVCD1_30'] < 0, 'SUMVCEREVCD1_30'] = 0

data3['SUMVCEREVCD1_30'].describe()

data3.isnull().sum().sort_values()

data3.shape

"""<h3> Filling Null Values of DeviceNetwork </h3>"""

data3[(data3['DEVICETYPE']=='Voice Centric') & (data3['ISDEVICE3GENABLED']=='Y') & (data3['DEVICENETWORK'].isnull())]

data3.loc[ (data3['DEVICETYPE']=='Voice Centric') & (data3['ISDEVICE3GENABLED']=='Y'), 'DEVICENETWORK']=data3["DEVICENETWORK"].fillna('3G')
data3.isnull().sum().sort_values()

data3[(data3['DEVICETYPE']=='Smartphone') & (data3['ISDEVICE3GENABLED']=='N') & (data3['DEVICENETWORK'].isnull())  &
      (data3['SUMDATAUSG4G']==0)]

data3.loc[ (data3['DEVICETYPE']=='Smartphone') & (data3['ISDEVICE3GENABLED']=='N') & (data3['SUMDATAUSG4G']==0), 'DEVICENETWORK']=data3["DEVICENETWORK"].fillna('2G')
data3.isnull().sum().sort_values()

data3.shape

"""<h2> Encoding </h2>"""

data5=data3.copy(deep=True)

data5

data5=data5.dropna(subset='DEVICENETWORK',axis='index')
data5

#data5.to_csv('final_data.csv',index=False)

data6=data5.copy(deep=True)

data3

from sklearn.preprocessing import LabelEncoder

le=LabelEncoder()
data5['ISDEVICE3GENABLED_enco']=le.fit_transform(data5['ISDEVICE3GENABLED'])
data5['VASSUBSCRIBERFLAG_enco']=le.fit_transform(data5['VASSUBSCRIBERFLAG'])
data5['ISDEVICEDATAENABLED_enco']=le.fit_transform(data5['ISDEVICEDATAENABLED'])
data5['SMARTPHONEFLAG_enco']=le.fit_transform(data5['SMARTPHONEFLAG'])
data5.drop(['ISDEVICE3GENABLED','VASSUBSCRIBERFLAG','ISDEVICEDATAENABLED','SMARTPHONEFLAG'], axis=1, inplace=True)
data5

"""<h3> One-Hot Encoding </h3>"""

o_h_e_1 = pd.get_dummies(data5['DEVICENETWORK'], prefix='network',dtype=int)
o_h_e_2=pd.get_dummies(data5['DEVICETYPE'],prefix='DEVICETYPE',dtype=int)
o_h_e_3=pd.get_dummies(data5['CUSTSEGMENT'],prefix='CUSTSEGMENT',dtype=int)
# Concatenate the one-hot encoded columns back to the original DataFrame
data5 = pd.concat([data5, o_h_e_1], axis=1)
data5=pd.concat([data5,o_h_e_2],axis=1)
data5=pd.concat([data5,o_h_e_3],axis=1)
# Drop the original 'network_technology' column as it's no longer needed
data5.drop('DEVICENETWORK', axis=1, inplace=True)
data5.drop('DEVICETYPE', axis=1, inplace=True)
data5.drop('CUSTSEGMENT', axis=1, inplace=True)

data5

data5=data5.dropna(subset='LASTRECHRGDATE_days',axis='index')

data5.isnull().sum()

data5.drop(['null_value_cols','network_custsegment'],axis=1,inplace=True)
data5

"""<h2> Model Making </h2>

<h2> Model 1</h2>

## Implicit Recommendation System

An implicit recommendation system is a type of recommendation system that infers user preferences from observed behavior, without explicit feedback from the user. Unlike explicit recommendation systems, where users provide direct feedback such as ratings or likes, implicit recommendation systems rely on indirect signals. These signals can include actions like clicks, purchase history, browsing time, search patterns, and other forms of user interaction with items.

### 1. Data Source:
Implicit recommendation systems use data that is naturally collected during user interactions. This can include:
- Clicks or views on products or articles.
- Time spent on a particular page or item.
- Purchase history.
- Search queries and navigation patterns.

### 2. Preference Inference:
Since there's no direct feedback, the system must infer user preferences from these interactions. This can be more complex, as the absence of an interaction doesn't necessarily mean a dislike, and the meaning of different interactions can vary widely.

### 3. Algorithms:
Algorithms for implicit recommendation often need to handle large amounts of sparse and noisy data. Collaborative filtering, matrix factorization, and deep learning models can be adapted for implicit feedback. Libraries like Implicit (a Python library specifically designed for implicit feedback datasets) can be used to build these models.

### 4. Challenges:
- **Cold Start Problem**: Implicit systems can struggle with new users or items that have little interaction data.
- **Noise**: Interactions like clicks may not always indicate a strong preference and can introduce noise into the data.
- **Scalability**: Handling large amounts of interaction data can be computationally challenging.

### 5. Applications:
Implicit recommendation systems are widely used in e-commerce, content recommendation (e.g., news, videos), music streaming, and other domains where explicit feedback is sparse or not available.

### Conclusion:
Implicit recommendation systems offer a powerful way to personalize experiences based on observed behavior, without needing explicit feedback. They can be more complex to build and interpret, but they leverage the rich interaction data that is often readily available. They are particularly useful in scenarios where collecting explicit feedback is challenging or not feasible.
"""

#!pip install implicit
import implicit
import pandas as pd
from scipy.sparse import coo_matrix

df=data6.copy(deep=True)
df = df.drop_duplicates(subset='UserID', keep='first')
df['VASSUBSCRIBERFLAG_enco']=le.fit_transform(df['VASSUBSCRIBERFLAG'])
df=df.drop(['VASSUBSCRIBERFLAG'],axis=1).copy(deep=True)
df=df.dropna(subset='LASTRECHRGDATE_days',axis='index').copy(deep=True)
# Combine the DEVICENETWORK and CUSTSEGMENT columns to create a new item column
df['ITEM'] = df['DEVICENETWORK'] + '_' + df['CUSTSEGMENT']

# Create a new DataFrame to represent the interactions
interactions_df = df[['UserID', 'ITEM', 'SUMDATAUSGCD1_30', 'SUMVOICEUSGCD1_30', 'ARPUD1_30', 'VASSUBSCRIBERFLAG_enco']]
# We are not putting SUMDATAREV_30 , SUMVOICEREV_30 since for new customer we will have 0 in both these columns these could effect the recommendation.
# Normalizing the columns
interactions_df['SUMDATAUSGCD1_30'] = (interactions_df['SUMDATAUSGCD1_30'] - interactions_df['SUMDATAUSGCD1_30'].min()) / (interactions_df['SUMDATAUSGCD1_30'].max() - interactions_df['SUMDATAUSGCD1_30'].min())
interactions_df['SUMVOICEUSGCD1_30'] = (interactions_df['SUMVOICEUSGCD1_30'] - interactions_df['SUMVOICEUSGCD1_30'].min()) / (interactions_df['SUMVOICEUSGCD1_30'].max() - interactions_df['SUMVOICEUSGCD1_30'].min())
interactions_df['ARPUD1_30'] = (interactions_df['ARPUD1_30'] - interactions_df['ARPUD1_30'].min()) / (interactions_df['ARPUD1_30'].max() - interactions_df['ARPUD1_30'].min())

# Creating a combined score
interactions_df['INTERACTION_SCORE'] = interactions_df['SUMDATAUSGCD1_30'] + interactions_df['SUMVOICEUSGCD1_30'] + interactions_df['ARPUD1_30'] + interactions_df['VASSUBSCRIBERFLAG_enco']

from sklearn.model_selection import train_test_split

# Assuming interactions_df is your DataFrame containing the interactions
# Split the DataFrame into training and validation sets
train_df, val_df = train_test_split(interactions_df, test_size=0.2, random_state=42)

# Create user-item matrices for the training and validation sets
def create_user_item_matrix(df):
    users = df['UserID'].astype("category")
    items = df['ITEM'].astype("category")
    interactions = df['INTERACTION_SCORE']
    users_codes = users.cat.codes + 1
    return coo_matrix((interactions, (users_codes, items.cat.codes)), shape=(users_codes.max() + 1, items.nunique())).tocsr()

user_item_matrix_train = create_user_item_matrix(train_df)
user_item_matrix_val = create_user_item_matrix(val_df)

"""<pre>
from implicit.evaluation import precision_at_k

# Define the hyperparameter grid
factors_grid = [10, 50, 100, 150, 200]
regularization_grid = [0.001, 0.01, 0.1, 0.5, 1]
iterations_grid = [10, 20, 50, 100, 150]

# Split the data into training and validation sets
# ...

# Keep track of the best parameters
best_params = None
best_precision = 0

# Iterate over the hyperparameter grid
for factors in factors_grid:
    for regularization in regularization_grid:
        for iterations in iterations_grid:
            # Initialize and train the ALS model
            model = implicit.als.AlternatingLeastSquares(factors=factors, regularization=regularization, iterations=iterations)
            model.fit(user_item_matrix_train)

            # Evaluate the model on the validation set
            precision = precision_at_k(model, user_item_matrix_train, user_item_matrix_val, K=10)
            
            print("Precision found for factor ",factors ," regularization",regularization,' iterations',iterations,' is',precision)
            
            # Update the best parameters if this model is better
            if precision > best_precision:
                best_precision = precision
                best_params = (factors, regularization, iterations)
                

# Print the best parameters
print("Best parameters:", best_params)
</pre>

![implicit_hyperparameter_tunning.png]

| Rank | Factor | Regularization | Iterations | Precision               |
|------|--------|----------------|------------|-------------------------|
| 1    | 150    | 0.5            | 50         | 0.7001471670345842      |
| 2    | 150    | 0.5            | 100        | 0.6889461205134494      |
| 3    | 200    | 0.5            | 100        | 0.686370697408225       |
| 4    | 200    | 0.5            | 50         | 0.683386476984711       |
| 5    | 100    | 0.5            | 100        | 0.6796664213882757      |
| 6    | 100    | 0.5            | 50         | 0.6706320006540757      |
| 7    | 50     | 0.5            | 50         | 0.6371515002861581      |
| 8    | 50     | 0.5            | 100        | 0.6243561442236939      |
| 9    | 200    | 0.5            | 20         | 0.6199002534543373      |
| 10   | 150    | 0.1            | 100        | 0.6064508216826098      |
"""

def personalised_best_offer(data,ss,user_id):
    d={
        '1G_No_ne': 0,
    '1G_Basic': 1,
        '1G_Silver': 2,
    '1G_Gold': 3,
    '1G_Platinum': 4,
    '2G_No_ne': 5,
    '2G_Basic': 6,
        '2G_Silver': 7,
    '2G_Gold': 8,
    '2G_Platinum': 9,
     '3G_No_ne': 10,
    '3G_Basic': 11,
        '3G_Silver': 12,
    '3G_Gold': 13,
    '3G_Platinum': 14,
    '4G_No_ne': 15,
    '4G_Basic': 16,
        '4G_Silver': 17,
    '4G_Gold': 18,
    '4G_Platinum': 19,
    '4G_Signature': 20,
    '5G_No_ne': 21,
    '5G_Basic': 22,
        '5G_Silver': 23,
    '5G_Gold': 24,
    '5G_Platinum': 25,
}
    dee={
    0: '1G_No_ne',
    1: '1G_Basic',
    2: '1G_Silver',
    3: '1G_Gold',
    4: '1G_Platinum',
    5: '2G_No_ne',
    6: '2G_Basic',
    7: '2G_Silver',
    8: '2G_Gold',
    9: '2G_Platinum',
    10: '3G_No_ne',
    11: '3G_Basic',
    12: '3G_Silver',
    13: '3G_Gold',
    14: '3G_Platinum',
    15: '4G_No_ne',
    16: '4G_Basic',
    17: '4G_Silver',
    18: '4G_Gold',
    19: '4G_Platinum',
    20: '4G_Signature',
    21: '5G_No_ne',
    22: '5G_Basic',
    23: '5G_Silver',
    24: '5G_Gold',
    25: '5G_Platinum'
}

    di= list(df[(df['UserID']==user_id) ]['DEVICENETWORK'])[0]
    if len(list(df[(df['UserID']==user_id) ]['DEVICENETWORK']))>0:
        cus=list(df[(df['UserID']==user_id) ]['CUSTSEGMENT'])[0]
    else:
        cus='No_ne'
    s=di+"_"+cus
    num=d[s]
    ki=list()
    if num+3 <=25 and num >d[ss]:
        for i in range(num,num+4,1):
            nh=dee[i]
            ki.append(nh)
        return ki
    elif num+3>25 and num>dee[ss]:
        for i in range(num,num+3-25+num,1):
            nh=dee[i]
            ki.append(nh)
        return ki

def print_recomendation(item_ids, scores,user_id):
    d = dict(zip(item_ids, scores))
    dii = {
    '1G_Basic': 0,
    '1G_Gold': 1,
    '1G_No_ne': 2,
    '1G_Platinum': 3,
    '1G_Silver': 4,
    '2G_Basic': 5,
    '2G_Gold': 6,
    '2G_No_ne': 7,
    '2G_Platinum': 8,
    '2G_Silver': 9,
    '3G_Basic': 10,
    '3G_Gold': 11,
    '3G_No_ne': 12,
    '3G_Platinum': 13,
    '3G_Silver': 14,
    '4G_Basic': 15,
    '4G_Gold': 16,
    '4G_No_ne': 17,
    '4G_Platinum': 18,
    '4G_Signature': 19,
    '4G_Silver': 20,
    '5G_Basic': 21,
    '5G_Gold': 22,
    '5G_No_ne': 23,
    '5G_Platinum': 24,
    '5G_Silver': 25
}

    for item_id, score in zip(item_ids,scores):
        if max(scores)==score:
            #print(f"Most Recommended Item: {items.cat.categories[item_id]} with score: {score}")
            ss=items.cat.categories[item_id]
    l=personalised_best_offer(data,ss,user_id)
    c=0
    sii=''
    for i in l:
        if dii[i] in d and c<4:
            ss=i
            if ss[3:]=='No_ne':
                ss=ss[:3]+"Basic"
            print(f"Personalised Recommended Item: {ss} with score: {d[dii[i]]}")
            c=c+1
            sii=sii+" "+i
    for i in list(item_ids):
        if str(d[i]) not in sii and i in d and c<6:
            ss=items.cat.categories[i]
            if ss[3:]=='No_ne':
                ss=ss[:3]+"Basic"
                if ss not in sii:
                    print(f" Other Recommended Item: {ss} with score: {d[i]}")
                    sii=sii+" "+ss
                    c=c+1

# ...
# Assuming df is your DataFrame


# Create a user-item interaction matrix
users = interactions_df['UserID'].astype("category")
items = interactions_df['ITEM'].astype("category")
interactions = interactions_df['INTERACTION_SCORE']

# Map the user IDs to a continuous range starting from 1
users_codes = users.cat.codes + 1

# Create a sparse matrix with one row for each unique user
user_item_matrix_coo = coo_matrix((interactions, (users_codes, items.cat.codes)), shape=(users_codes.max() + 1, items.nunique()))

# Convert to CSR format
user_item_matrix = user_item_matrix_coo.tocsr()

# Initialize the ALS model
model = implicit.als.AlternatingLeastSquares(factors=150, regularization=0.5, iterations=50)

# Train the model
model.fit(user_item_matrix)

# Generate recommendations for a specific user
user_id = 5 # or any other valid user ID
recommended =  model.recommend(user_id, user_item_matrix[user_id],N=10)

# Print the recommendations
item_ids, scores = recommended


print_recomendation(item_ids, scores,user_id)

from implicit.evaluation import precision_at_k

print("Precision is ",precision_at_k(model, user_item_matrix_train, user_item_matrix_val, K=15))

"""<h2>Model 2</h2>

### PCA Data Transformation Analysis for Clustering

In the process of dimensionality reduction using Principal Component Analysis (PCA), different data transformation strategies were employed. The following table summarizes the results:

### PCA Data Transformation Analysis for Clustering

In the process of dimensionality reduction using Principal Component Analysis (PCA), different data transformation strategies were employed. The following table summarizes the results:

|Data Transformation    | Cumulative Explained Variance Threshold Number| Kaiser Criterion Number|
|-----------------------|-----------------------------------------------|------------------------|
| No Transformation     | 1                                             | 10                     |
| StandardScaler        | 18                                            | 11                     |
| MinMaxScaler          | 9                                             | 1                      |
| RobustScaler          | 1                                             | 7                      |
| np.log1p              | 3                                             | 5                      |


#### Analysis

1. **No Transformation**: Retains 10 components based on the Kaiser Criterion but only explains a minimal amount of variance.

2. **StandardScaler**: Provides a good balance between dimensionality reduction and information retention.

3. **MinMaxScaler**: Might be too restrictive with only 1 component explaining the variance.

4. **RobustScaler**: Also possibly too restrictive with 1 component explaining the variance.

5. **np.log1p**: Captures more complex structures but could also include noise.





#### Recommendation

Based on this analysis, the **StandardScaler** transformation seems to provide a suitable choice for clustering. It retains enough components to explain the variance in the data without including too much noise, making it potentially effective for capturing underlying patterns.

However, the best transformation strategy might also depend on the specific characteristics of the data and the clustering algorithm in use. It is advisable to perform clustering with different transformations and evaluate the results using clustering metrics to find the transformation that gives the best clustering performance for the specific dataset.



from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

def pca_analysis(X):
    pca = PCA().fit(X)
    fig = px.line(x=range(1, len(pca.explained_variance_ratio_) + 1),
              y=np.cumsum(pca.explained_variance_ratio_))
    fig.update_layout(title='Cumulative Explained Variance',
                  xaxis_title='Number of Components',
                  yaxis_title='Cumulative Explained Variance')
    fig.show()
    pca = PCA(0.95).fit(X)
    X_pca = pca.transform(X)
    print("Cumulative Explained Variance Threshold ","Number of components:", pca.n_components_)

    pca = PCA().fit(X)

    # Find the number of components with eigenvalues > 1
    num_components = sum(pca.explained_variance_ > 1)

    # Perform PCA with the selected number of components
    pca = PCA(n_components=num_components)
    X_pca = pca.fit_transform(X)

    print(f"Kaiser Criterion Number of components retained: {num_components}")
    pca = PCA().fit(X)
    fig = px.scatter(x=range(1, len(pca.explained_variance_) + 1),
                 y=pca.explained_variance_,
                 title='Scree Plot',
                 labels={'x': 'Number of Components', 'y': 'Explained Variance (Eigenvalue)'})
    fig.update_traces(mode='lines+markers', line=dict(dash='dash'))
    fig.show()

pca_analysis(X)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)
pca_analysis(X_standardized)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
pca_analysis(X_scaled)

from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
X_robust = scaler.fit_transform(X)
pca_analysis(X_robust)

import numpy as np

X_log = np.log1p(X)  # log1p helps manage zeros by adding 1 before taking the log
pca_analysis(X_log)

from sklearn.preprocessing import QuantileTransformer

transformer = QuantileTransformer(output_distribution='normal')
X_quantile = transformer.fit_transform(X)
pca_analysis(X_quantile)

"""<h2> Model 1 </h2>

# Clustering Model Evaluation Report

In this report, we evaluate the performance of a clustering model on a dataset, both before and after normalization. The performance is evaluated using three metrics: the Silhouette Score, the Davies-Bouldin Index, and the Calinski-Harabasz Index.

## Methodology

The clustering model was applied to the dataset both before and after normalization. Normalization is a preprocessing step that scales the features to have zero mean and unit variance, ensuring that all features contribute approximately proportionately to the final distance between instances.

## Results

The performance of the clustering model on the normalized and non-normalized data is summarized in the following table:

| Data Type      | Silhouette Score | Davies-Bouldin Index | Calinski-Harabasz Index |
|---------------|------------------|----------------------|-------------------------|
| Normalized    | 0.2132           | 2.3694               | 10377.3451              |
| Non-Normalized| 0.8546           | 0.5881               | 578794.0567             |

## Interpretation

* The Silhouette Score, which ranges from -1 (worst) to 1 (best), measures how close each point in one cluster is to the points in the neighboring clusters.
* The Davies-Bouldin Index measures the average similarity between each cluster and its most similar cluster, with lower values indicating better separation between clusters.
* The Calinski-Harabasz Index measures the ratio of between-cluster dispersion to within-cluster dispersion, with higher values indicating better clustering.

In this case, all three metrics suggest that the clustering model performed better before normalization. The Silhouette Score decreased from 0.8546 to 0.2132, the Davies-Bouldin Index increased from 0.5881 to 2.3694, and the Calinski-Harabasz Index decreased from 578794.0567 to 10377.3451.

## Conclusion

While these results suggest that the clustering model performed better before normalization, it's important to remember that these are just quantitative measures and may not fully capture the quality of the clustering, especially if there are other domain-specific considerations that these metrics don't take into account. It's also possible that the number of clusters chosen for the K-means algorithm is not optimal for the normalized data. Further investigation and experimentation may be necessary to fully understand the performance of the clustering model on this dataset.
"""

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Assume df is your DataFrame
X = data5.values

# Perform PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Perform K-means clustering
kmeans = KMeans(n_clusters=10)  # or however many clusters you want
kmeans.fit(X_pca)

# Get the cluster assignments for each data point
labels = kmeans.labels_

# Plot the data points with color coding for the clusters
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels)
plt.show()

import plotly.express as px
import pandas as pd

# Assume df is your DataFrame
X = data5.values

# Perform PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Perform K-means clustering
kmeans = KMeans(n_clusters=10)  # or however many clusters you want
kmeans.fit(X_pca)

# Get the cluster assignments for each data point
labels = kmeans.labels_

# Create a DataFrame for the PCA-transformed data
df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
df_pca['Cluster'] = labels

# Create the scatter plot
fig = px.scatter(df_pca, x='PC1', y='PC2', color='Cluster')
fig.show()



import plotly.express as px
import pandas as pd

# Assume df is your DataFrame
X = data5.values

# Perform PCA
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X)

# Perform K-means clustering
kmeans = KMeans(n_clusters=10)  # or however many clusters you want
kmeans.fit(X)

# Get the cluster assignments for each data point
labels = kmeans.labels_
# Create a DataFrame for the PCA-transformed data
df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2', 'PC3'])
df_pca['Cluster'] = labels

# Create the 3D scatter plot
fig = px.scatter_3d(df_pca, x='PC1', y='PC2', z='PC3', color='Cluster')
fig.show()

"

from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# Assume labels are the predicted labels from your clustering model
print("Silhouette Score: ", silhouette_score(X_pca, labels))
print("Davies-Bouldin Index: ", davies_bouldin_score(X_pca, labels))
print("Calinski-Harabasz Index: ", calinski_harabasz_score(X_pca, labels))

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Assume X is your feature matrix
silhouette_scores = []
inertia_scores = []

# Define the range of k values to try
k_values = range(2, 11)

for k in k_values:
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(X_pca)
    labels = kmeans.labels_

    # Calculate silhouette score and add it to the list
    silhouette_scores.append(silhouette_score(X_pca, labels))

    # Calculate inertia (sum of squared distances to closest centroid) and add it to the list
    inertia_scores.append(kmeans.inertia_)

# Plot silhouette scores
plt.figure(figsize=(8, 4))
plt.plot(k_values, silhouette_scores, 'bo-')
plt.xlabel('k (number of clusters)')
plt.ylabel('Silhouette Score')
plt.show()

# Plot inertia scores
plt.figure(figsize=(8, 4))
plt.plot(k_values, inertia_scores, 'bo-')
plt.xlabel('k (number of clusters)')
plt.ylabel('Inertia')
plt.show()



#data5.to_csv('final_data.csv',index=False)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import plotly.express as px
import pandas as pd
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# Assume df is your DataFrame
X = data5.values

# Normalize the data
scaler = StandardScaler()
X_normalized = scaler.fit_transform(X)

# Perform PCA
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_normalized)

# Perform K-means clustering
kmeans = KMeans(n_clusters=10)  # or however many clusters you want
kmeans.fit(X_pca)

# Get the cluster assignments for each data point
labels = kmeans.labels_

# Create a DataFrame for the PCA-transformed data
df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2', 'PC3'])
df_pca['Cluster'] = labels
# Define a list of colors
colors = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'pink', 'lime', 'olive', 'cyan']
# Create the 3D scatter plot
fig = px.scatter_3d(df_pca, x='PC1', y='PC2', z='PC3', color='Cluster',color_discrete_sequence=colors)

# Update the layout to increase the size of the plot
fig.update_layout(width=800, height=800)

fig.show()

# Print the clustering metrics
print("Silhouette Score: ", silhouette_score(X_normalized, labels))
print("Davies-Bouldin Index: ", davies_bouldin_score(X_normalized, labels))
print("Calinski-Harabasz Index: ", calinski_harabasz_score(X_normalized, labels))



data5.columns

"

data3['CUSTSEGMENT'].value_counts().to_dict()

def fun(data):
    l = list(data.columns)
    m=list()
    h=list()
    for i in l:
        if data[i].dtypes=='object':
                g=list(data[i].value_counts().to_dict().keys())
                k=list()
                k.append(i)
                k.append(g)
                h.append(k)
        else:
            g=list()
            k=list()
            k.append(data[i].min())
            k.append(data[i].max())
            g.append(i)
            g.append(k)
            h.append(tuple(g))
    return h
fun(data3)

data6

data6=data6.drop(['null_value_cols'],axis=1).copy(deep=True)

data6

# Convert the 'ITEM' column to a categorical variable
df['ITEM'] = df['ITEM'].astype('category')

# Create a new column 'ITEM_ID' that contains the category codes for the 'ITEM' column
df['ITEM_ID'] = df['ITEM'].cat.codes

# Now, the 'ITEM_ID' column represents a unique numerical identifier for each unique combination of 'DEVICENETWORK' and 'CUSTSEGMENT'
print(df[['DEVICENETWORK', 'CUSTSEGMENT', 'ITEM', 'ITEM_ID']].head())

df.isnull().sum()

print(list(item_ids))

print(recommended)

ss="1G_No_ne"
print(ss[3:])
if ss[3:]=='No_ne':
    ss=ss[:3]+"Basic"
ss

df["UserID"][7]==5

list(df[(df['UserID']==5) ]['DEVICENETWORK'])[0]

for i in range(26):
    print(items.cat.categories[i],i)

u_i_m=user_item_matrix[:].toarray()

for i in range(len(u_i_m)):
    for j in range(len(u_i_m[0])) :
        if u_i_m[i][j] ==max(u_i_m[i,:]):
            u_i_m[i][j]=1



u_i_m[1,:]

u_i_m.shape[0]

u_i_m=np.array(u_i_m)

u_i_m[3].nonzero()[0][0]

"""from sklearn.metrics import precision_score, recall_score

def precision_recall_at_k(model, u_i_m, user_item_matrix,k=10):
    precisions = []
    recalls = []
    l=list()
    for i in range(u_i_m.shape[0]):
        g=list()
        g.append(u_i_m[i].nonzero()[0][0])
        l.append(g)
    # Assuming user_item_matrix is a binary matrix where 1 indicates an interaction
    for user_id in range(u_i_m.shape[0]):
        # Get the true positive items for this user
        #print(user_id)
        true_positives = l[user_id]
        
        # Skip users with no interactions
        if len(true_positives) == 0:
            continue
        
        # Get the recommendations for this user
        recommendations = model.recommend(user_id, user_item_matrix[user_id], N=k)
        recommended_items = [item[0] for item in recommendations]
        
        # Compute precision and recall
        relevant_recommendations = len(set(recommended_items) & set(true_positives))
        precision = relevant_recommendations / k
        recall = relevant_recommendations / len(true_positives)
        
        precisions.append(precision)
        recalls.append(recall)
        
    return np.mean(precisions), np.mean(recalls)

# Assuming model is your trained ALS model and user_item_matrix is your user-item interaction matrix
precision, recall = precision_recall_at_k(model,u_i_m ,user_item_matrix,k=10)
print(f"Precision@{10}: {precision}")
print(f"Recall@{10}: {recall}")

"""

interactions_df['INTERACTION_SCORE'].describe()

interactions_df.shape

print(user_item_matrix.shape)

unique_items = df['ITEM'].nunique()
print(unique_items)  # This should match the number of columns in the sparse matrix

sparsity = user_item_matrix.count_nonzero() / (user_item_matrix.shape[0] * user_item_matrix.shape[1])
print(f"Sparsity: {sparsity * 100:.2f}%")

print(user_item_matrix[7, :].toarray())

item_id = 1 # Since Python uses 0-based indexing, the 18th column corresponds to index 17
item_name = items.cat.categories[item_id]
print(f"The interaction corresponds to the item: {item_name}")

user_item_matrix[user_item_matrix.nonzero()]

user_factors = model.user_factors
item_factors = model.item_factors
user_factors

user_item_matrix.shape[0]

user_count = 1 if np.isscalar(user_id) else len(user_id)
user_count

df

df.duplicated()

mobile_to_user_id

df['USERID']=df.index
df=df.drop(['USERID'],axis=1)
df.head(10)

df['UserID'][0]

# Assuming df is your DataFrame
df = df.drop_duplicates(subset='UserID', keep='first')
